## Problema da Ordenação

 **Entrada:** Uma sequência de inteiros a<sub>1</sub>,...,a<sub>n</sub> e i tal que 1 ≤ i ≤ n\
 **Saída:** Uma permutação da entrada a′<sub>1</sub>,...a′<sub>n</sub> tal que a<sub>i</sub> ≤ a<sub>j</sub> sempre que i < j.
 
 Escolha dois algoritmos de ordenação vistos em aula. Implemente os dois em C, Java ou Python. Compute o tempo total de processamento para 100 entradas. Repita o processo para entradas de tamanhos diferentes. Por fim, escreva um relatório curto (até 4 páginas) indicando o resultado do experimento. Qual era o modelo do tempo de processamento no pior caso de cada um deles? Qual dos dois foi mais eficiente? O resultado obtido corresponde ao modelo teórico? Para gerar os testes vocês podem usar o programa disponibilizado em https://github.com/marciomr/IAA/blob/main/gerador.c.